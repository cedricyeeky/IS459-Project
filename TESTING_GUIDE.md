# Testing Guide for Data Cleaning Updates

## ‚úÖ Tests Already Passed

### Logic Validation Test
```bash
python3 test_logic_simple.py
```
**Status:** ‚úÖ **PASSED** - All deduplication logic validated

---

## Testing Options

### Option 1: Simple Logic Test (NO DEPENDENCIES) ‚ö° **FASTEST**
**What it tests:** Column detection and key filtering logic  
**Requires:** Nothing! Just Python 3  
**Time:** ~1 second

```bash
python3 test_logic_simple.py
```

‚úÖ **Already validated and passed!**

---

### Option 2: Local PySpark Test (RECOMMENDED)
**What it tests:** Full deduplication with actual weather data  
**Requires:** PySpark installation  
**Time:** ~30 seconds

#### Setup:
```bash
pip install pyspark
```

#### Run:
```bash
python3 test_data_cleaning.py
```

#### What to expect:
```
TEST 1: Weather Data Deduplication
‚úÖ Weather data test passed!

TEST 2: Flight Data Deduplication  
‚úÖ Flight data test passed!

TEST 3: Missing Columns Handling
‚úÖ Missing columns test passed!

TEST 4: All Columns Deduplication
‚úÖ All columns deduplication test passed!

TEST 5: Supplemental Data Type Detection
‚úÖ Auto-detection test passed!

‚úÖ ALL TESTS PASSED!
```

---

### Option 3: AWS Glue Test (PRODUCTION VALIDATION)
**What it tests:** Actual AWS Glue job execution  
**Requires:** AWS credentials, Terraform deployed infrastructure  
**Time:** ~5-10 minutes

#### Step 1: Upload Sample Data
```bash
# Edit test_glue_weather.sh and update:
# - RAW_BUCKET="your-raw-bucket-name"
# - REGION="your-region"

./test_glue_weather.sh
```

#### Step 2: Trigger Glue Job
Either:
- **Manual:** Run Glue job from AWS Console
- **Automated:** Use Terraform or AWS CLI

```bash
aws glue start-job-run \
    --job-name your-data-cleaning-job \
    --region us-east-1
```

#### Step 3: Check CloudWatch Logs
Look for these messages in CloudWatch:
```
‚úÖ "Detected weather data format"
‚úÖ "Deduplicating using columns: ['obs_id', 'valid_time_gmt']"
‚úÖ "Removed X duplicate records"
‚úÖ "Supplemental data processing complete"
```

#### Step 4: Verify Output
```bash
# Check Silver bucket for processed data
aws s3 ls s3://your-silver-bucket/ --recursive

# Download and inspect
aws s3 cp s3://your-silver-bucket/data.parquet /tmp/ --recursive
```

---

## Test Scenarios Covered

| Scenario | Test Method | Status |
|----------|-------------|--------|
| Weather data column detection | Logic test | ‚úÖ Passed |
| Weather deduplication keys | Logic test | ‚úÖ Passed |
| Missing columns handling | Logic test | ‚úÖ Passed |
| Flight data (shouldn't use weather keys) | Logic test | ‚úÖ Passed |
| Actual weather CSV reading | PySpark test | ‚è≥ Pending |
| Full deduplication on real data | PySpark test | ‚è≥ Pending |
| AWS Glue execution | AWS test | ‚è≥ Pending |
| CloudWatch logging | AWS test | ‚è≥ Pending |

---

## What Each Test Validates

### ‚úÖ Logic Test (Completed)
- Column detection works (`obs_id`, `valid_time_gmt`)
- Key filtering removes non-existent columns
- Doesn't false-positive on flight data

### ‚è≥ PySpark Test (Optional - Requires Setup)
- Reads actual CSV files correctly
- Deduplication removes duplicates
- Schema inference works
- All data types handled

### ‚è≥ AWS Glue Test (Required Before Production)
- S3 read/write operations
- Glue job execution
- Error handling and DLQ
- CloudWatch logging
- Performance at scale

---

## Quick Start Recommendation

**For immediate validation:**
```bash
# Already passed! ‚úÖ
python3 test_logic_simple.py
```

**Before deploying to AWS:**
```bash
# 1. Install PySpark (optional but recommended)
pip install pyspark

# 2. Run full local test
python3 test_data_cleaning.py

# 3. Deploy to AWS and run sample test
./test_glue_weather.sh
# Then trigger Glue job and check CloudWatch
```

---

## Troubleshooting

### If PySpark test fails:
```bash
# Check PySpark installation
python3 -c "import pyspark; print(pyspark.__version__)"

# If import fails, reinstall:
pip install --upgrade pyspark
```

### If AWS test fails:
- Check S3 bucket names in script
- Verify AWS credentials: `aws sts get-caller-identity`
- Check Glue job exists: `aws glue get-job --job-name <name>`
- Review CloudWatch logs for errors

---

## Expected Results

### Weather Data Processing:
- **Input:** 822,212 rows from `weather_data_list1.csv`
- **Columns:** `obs_id`, `valid_time_gmt`, `wx_phrase`, `temp`, etc.
- **Dedup Keys:** `obs_id`, `valid_time_gmt`
- **Output:** Deduplicated Parquet in Silver bucket

### Log Messages to Look For:
```
‚úÖ "Read X records from supplemental CSV data"
‚úÖ "Detected weather data format"  
‚úÖ "Deduplicating using columns: ['obs_id', 'valid_time_gmt']"
‚úÖ "Removed X duplicate records"
‚úÖ "Supplemental data processing complete: Y records"
‚úÖ "Successfully wrote Y records to Silver bucket"
```

### Red Flags üö©:
```
‚ùå "None of the specified key columns ... exist in DataFrame"
‚ùå "KeyError: 'Year'" or similar
‚ùå "Error processing supplemental data"
```

If you see red flags, the code still has issues!

---

## Next Steps After Testing

1. ‚úÖ Logic validation passed
2. ‚è≥ Run PySpark test (optional but recommended)
3. ‚è≥ Deploy updated `data_cleaning.py` to AWS
4. ‚è≥ Upload weather data to S3 raw bucket
5. ‚è≥ Run Glue job and monitor CloudWatch
6. ‚è≥ Verify output in Silver bucket
7. ‚úÖ Production ready!
